{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import dump\n",
    "import pickle as pkl\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, precision_recall_curve\n",
    "from sklearn.metrics import make_scorer, fbeta_score,  mean_squared_error, r2_score, f1_score\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, precision_recall_curve, classification_report, confusion_matrix)\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, GridSearchCV, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, plot_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f2_tuned_scaled_xgb:\n",
    "Selected features: ['Fwd IAT Total' 'Fwd Packet Length Max' 'Bwd Packet Length Mean'\n",
    " 'Bwd Packet Length Std' 'Bwd Packets Length Total'\n",
    " 'Bwd Packet Length Max' 'Packet Length Max' 'Packet Length Mean'\n",
    " 'Packet Length Std' 'Packet Length Variance' 'Avg Packet Size'\n",
    " 'Fwd Header Length' 'Avg Fwd Segment Size']\n",
    "\n",
    " f2_all_features:\n",
    " Selected features: ['Protocol' 'Flow Duration' 'Total Fwd Packets' 'Total Backward Packets'\n",
    " 'Fwd Packets Length Total' 'Bwd Packets Length Total'\n",
    " 'Fwd Packet Length Max' 'Fwd Packet Length Min' 'Fwd Packet Length Mean'\n",
    " 'Fwd Packet Length Std' 'Bwd Packet Length Max' 'Bwd Packet Length Min'\n",
    " 'Bwd Packet Length Mean' 'Bwd Packet Length Std' 'Flow Bytes/s'\n",
    " 'Flow Packets/s' 'Flow IAT Mean' 'Flow IAT Std' 'Flow IAT Max'\n",
    " 'Flow IAT Min' 'Fwd IAT Total' 'Fwd IAT Max' 'Fwd IAT Min'\n",
    " 'Bwd IAT Total' 'Bwd IAT Mean' 'Bwd IAT Std' 'Bwd IAT Min'\n",
    " 'Fwd PSH Flags' 'Fwd Header Length' 'Bwd Header Length'\n",
    " 'Packet Length Min' 'Packet Length Max' 'Packet Length Mean'\n",
    " 'Packet Length Std' 'PSH Flag Count' 'ACK Flag Count' 'URG Flag Count'\n",
    " 'Down/Up Ratio' 'Avg Packet Size' 'Init Fwd Win Bytes'\n",
    " 'Init Bwd Win Bytes' 'Fwd Act Data Packets' 'Fwd Seg Size Min'\n",
    " 'Active Std' 'Active Min' 'Idle Mean']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal features baseline rf: \n",
    "Selected features: ['Fwd IAT Total' 'Fwd Packet Length Max' 'Bwd Packet Length Mean'\n",
    " 'Bwd Packet Length Std' 'Bwd Packet Length Max' 'Packet Length Mean'\n",
    " 'Packet Length Std' 'Packet Length Variance' 'Avg Packet Size'\n",
    " 'Fwd Header Length']\n",
    "\n",
    "optimal features tuned rf:\n",
    "Selected features: ['Fwd IAT Total' 'Fwd Packet Length Max' 'Bwd Packet Length Mean'\n",
    " 'Bwd Packet Length Std' 'Bwd Packet Length Max' 'Packet Length Max'\n",
    " 'Packet Length Mean' 'Packet Length Std' 'Packet Length Variance'\n",
    " 'Avg Packet Size' 'Fwd Header Length' 'Avg Fwd Segment Size'\n",
    " 'Avg Bwd Segment Size']\n",
    "\n",
    "optimal features f2:\n",
    "    Selected features: ['Bwd Packet Length Mean' 'Bwd Packet Length Std' 'Packet Length Variance'\n",
    " 'Avg Packet Size']\n",
    "\n",
    "optimal features f2 scaled: \n",
    "Selected features: ['Bwd Packet Length Std' 'Packet Length Variance' 'Avg Packet Size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/work/SarahHvidAndersen#6681/DataScience_project/data/train_df.csv')\n",
    "eval_df = pd.read_csv('/work/SarahHvidAndersen#6681/DataScience_project/data/eval_df.csv')\n",
    "test_df = pd.read_csv('/work/SarahHvidAndersen#6681/DataScience_project/data/test_df.csv')\n",
    "\n",
    "\n",
    "features = ['Fwd IAT Total', 'Fwd Packet Length Max',\n",
    "        'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "        'Bwd Packets Length Total', 'Bwd Packet Length Max',\n",
    "        'Packet Length Max', 'Packet Length Mean', 'Packet Length Std',\n",
    "        'Packet Length Variance', 'Avg Packet Size', 'Fwd Header Length',\n",
    "        'Avg Fwd Segment Size', 'Avg Bwd Segment Size']\n",
    "\n",
    "\n",
    "#X_train = train_df[selected_features]  # Features from the training data\n",
    "X_train = train_df.drop(['Label', 'Attack'], axis=1)\n",
    "X_train_selected_feat = train_df[features]\n",
    "y_train = train_df['Attack'] \n",
    "\n",
    "#X_eval = eval_df[selected_features]  # Features from the evaluation data\n",
    "X_eval = eval_df.drop(['Label', 'Attack'], axis=1)\n",
    "y_eval = eval_df['Attack']\n",
    "\n",
    "#X_test = test_df[selected_features]\n",
    "X_test = test_df.drop(['Label', 'Attack'], axis=1)\n",
    "y_test = test_df['Attack']\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the features\n",
    "X_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])\n",
    "\n",
    "# Applying the 'trained' scaler on eval and test\n",
    "X_eval[X_eval.columns] = scaler.transform(X_eval[X_eval.columns])\n",
    "X_test[X_test.columns] = scaler.transform(X_test[X_test.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature reduction with crossvalidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summarized results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def load_object(file_path):\n",
    "    \"\"\"Load a GridSearchCV, RandomizedSearchCV, or RFECV object from a file.\"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        search = pickle.load(file)\n",
    "    return search\n",
    "\n",
    "def get_model_info(search):\n",
    "    \"\"\"Extract model type and features from the search object.\"\"\"\n",
    "    model_info = {\n",
    "        'Model Type': type(search.best_estimator_).__name__ if hasattr(search, 'best_estimator_') else type(search.estimator).__name__,\n",
    "        'Search Type': 'GridSearchCV' if isinstance(search, GridSearchCV) else ('RandomizedSearchCV' if isinstance(search, RandomizedSearchCV) else 'RFECV')\n",
    "    }\n",
    "    return model_info\n",
    "\n",
    "def summarize_search(search, model_name):\n",
    "    \"\"\"Summarize the search or RFECV results and return as a dictionary.\"\"\"\n",
    "    model_info = get_model_info(search)\n",
    "    \n",
    "    if model_info['Search Type'] == 'RFECV':\n",
    "        optimal_features_mask = search.support_\n",
    "\n",
    "        if 'all' in model_name:\n",
    "            feature_names = X_train.columns.tolist()\n",
    "            selected_features = np.array(feature_names)[optimal_features_mask]\n",
    "        else:\n",
    "            feature_names = X_train_selected_feat.columns.tolist()\n",
    "            selected_features = np.array(feature_names)[optimal_features_mask]\n",
    "        \n",
    "        summary = {\n",
    "            'Model': model_name,\n",
    "            ''\n",
    "            'Optimal Number of Features': search.n_features_,\n",
    "            'Selected Features': selected_features,\n",
    "            'Number of input features': len(optimal_features_mask),\n",
    "            'Model Type': model_info['Model Type'],\n",
    "            'Search Type': model_info['Search Type']\n",
    "        }\n",
    "    else:\n",
    "        summary = {\n",
    "            'Model': model_name,\n",
    "            'Best Parameters': search.best_params_,\n",
    "            'Best Score': search.best_score_,\n",
    "            'Scorer': search.scorer_,\n",
    "            'Features': model_info['Features'],\n",
    "            'Model Type': model_info['Model Type'],\n",
    "            'Search Type': model_info['Search Type']\n",
    "        }\n",
    "    return summary\n",
    "\n",
    "def load_and_summarize_searches(file_paths, model_names):\n",
    "    \"\"\"Load multiple search objects, summarize their results, and store in a DataFrame.\"\"\"\n",
    "    summaries = []\n",
    "    for file_path, model_name in zip(file_paths, model_names):\n",
    "        try:\n",
    "            search = load_object(file_path)\n",
    "            summary = summarize_search(search, model_name)\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {model_name} from {file_path}: {e}\")\n",
    "    return pd.DataFrame(summaries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# summarizing rfecv results\n",
    "file_paths = [\n",
    "    '../crossvalidation/rfecv_all_feat_RF.pkl',\n",
    "    '../crossvalidation/rfecv_selected_feat_RF.pkl',\n",
    "    '../crossvalidation/rfecv_all_feat_XGB.pkl',\n",
    "    '../crossvalidation/rfecv_selected_feat_XGB.pkl',\n",
    "]\n",
    "model_names = [\n",
    "    'Random Forest with all features',\n",
    "    'Random Forest with selected features',\n",
    "    'XGBoost with all features',\n",
    "    'XGBoost with selected features',\n",
    "]\n",
    "\n",
    "summaries_df = load_and_summarize_searches(file_paths, model_names)\n",
    "#summaries_df.to_csv('/work/SarahHvidAndersen#6681/DataScience_project/output/rfecv_results.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Optimal Number of Features</th>\n",
       "      <th>Selected Features</th>\n",
       "      <th>Number of input features</th>\n",
       "      <th>Model Type</th>\n",
       "      <th>Search Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest with all features</td>\n",
       "      <td>3</td>\n",
       "      <td>[Bwd Packet Length Std, Packet Length Std, Avg...</td>\n",
       "      <td>77</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>RFECV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest with selected features</td>\n",
       "      <td>3</td>\n",
       "      <td>[Bwd Packet Length Std, Packet Length Variance...</td>\n",
       "      <td>14</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>RFECV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost with all features</td>\n",
       "      <td>46</td>\n",
       "      <td>[Protocol, Flow Duration, Total Fwd Packets, T...</td>\n",
       "      <td>77</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>RFECV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost with selected features</td>\n",
       "      <td>13</td>\n",
       "      <td>[Fwd IAT Total, Fwd Packet Length Max, Bwd Pac...</td>\n",
       "      <td>14</td>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>RFECV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Model  Optimal Number of Features  \\\n",
       "0       Random Forest with all features                           3   \n",
       "1  Random Forest with selected features                           3   \n",
       "2             XGBoost with all features                          46   \n",
       "3        XGBoost with selected features                          13   \n",
       "\n",
       "                                   Selected Features  \\\n",
       "0  [Bwd Packet Length Std, Packet Length Std, Avg...   \n",
       "1  [Bwd Packet Length Std, Packet Length Variance...   \n",
       "2  [Protocol, Flow Duration, Total Fwd Packets, T...   \n",
       "3  [Fwd IAT Total, Fwd Packet Length Max, Bwd Pac...   \n",
       "\n",
       "   Number of input features              Model Type Search Type  \n",
       "0                        77  RandomForestClassifier       RFECV  \n",
       "1                        14  RandomForestClassifier       RFECV  \n",
       "2                        77           XGBClassifier       RFECV  \n",
       "3                        14           XGBClassifier       RFECV  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth = None, min_samples_leaf=2,\n",
    "                                        min_samples_split = 2, random_state=42)\n",
    "# better to use tuned version for feature selection, that should be optimal for all features already\n",
    "\n",
    "# Define the scorer, F2 Score, could be too high, maybe experiment with F1.5\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2, average='binary')\n",
    "\n",
    "# Create the RFE object and compute a cross-validated score.\n",
    "rfecv = RFECV(estimator=clf, step=1, cv=StratifiedKFold(5), scoring=f2_scorer)\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "# Optimal number of features : 10\n",
    "# tuned model Optimal number of features : 13\n",
    "# tuned for f2 score: Optimal number of features : 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all together\n",
    "filenames = ['../crossvalidation/rfecv_all_feat_RF.pkl',\n",
    "            '../crossvalidation/rfecv_selected_feat_RF.pkl',\n",
    "            '../crossvalidation/rfecv_all_feat_XGB.pkl',\n",
    "            '../crossvalidation/rfecv_selected_feat_XGB.pkl']\n",
    "\n",
    "\n",
    "# Load all RFECV objects\n",
    "rfecv_objects = []\n",
    "for filename in filenames:\n",
    "    with open(filename, 'rb') as file:\n",
    "        rfecv = pickle.load(file)\n",
    "        rfecv_objects.append(rfecv)\n",
    "\n",
    "# Plotting setup\n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "for i, rfecv in enumerate(rfecv_objects):\n",
    "    # Extract mean test scores\n",
    "    mean_scores = np.array(rfecv.cv_results_['mean_test_score'])\n",
    "    # Plot mean test scores\n",
    "    plt.plot(range(1, len(mean_scores) + 1), mean_scores, label=model_names[i])\n",
    "    # Highlight peak\n",
    "    peak_index = np.argmax(mean_scores)\n",
    "    plt.scatter(peak_index + 1, mean_scores[peak_index], color='red', zorder=5)\n",
    "\n",
    "# Enhancing the plot\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross-validation score (accuracy)\")\n",
    "plt.title('Feature Selection using RFECV')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the XGBoost classifier\n",
    "#xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb = XGBClassifier(objective='binary:logistic',use_label_encoder=False,\n",
    "                                colsample_bytree=1.0, gamma=0.5,learning_rate=0.3, max_depth=10,\n",
    "                                n_estimators=350, subsample=0.5, eval_metric='logloss')\n",
    "                                \n",
    "# Define the scorer, F2 Score, could be too high, maybe experiment with F1.5\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2, average='binary')\n",
    "# Create the RFE object with cross-validation\n",
    "rfecv = RFECV(estimator=xgb, step=1, cv=StratifiedKFold(5), scoring=f2_scorer)\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "# Extract mean test scores from cv_results_\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = XGBClassifier(objective='binary:logistic', use_label_encoder=False,\n",
    "colsample_bytree= 1.0, gamma = 0.5, learning_rate= 0.3, max_depth= 10, n_estimators= 350, subsample= 0.5, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "plot_importance(model, ax=ax, importance_type='weight') # weight, gain\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost fitted\n",
    "model = XGBClassifier(objective='binary:logistic',use_label_encoder=False,\n",
    "                                colsample_bytree=1.0, gamma=0.5,learning_rate=0.3, max_depth=10,\n",
    "                                n_estimators=350, subsample=0.5, eval_metric='logloss')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Define the F2 score as a custom scorer\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "# permutate\n",
    "result = permutation_importance(model, X_eval, y_eval, scoring=f2_scorer, n_repeats=10, random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_features = []\n",
    "\n",
    "# result.importances_mean provides the average decrease in model score due to the shuffling\n",
    "feature_importance = result.importances_mean\n",
    "for i in range(X_eval.shape[1]):\n",
    "    if feature_importance[i] > 0:\n",
    "        print(f\"Feature: {X_eval.columns[i]}, Importance: {feature_importance[i]}\")\n",
    "        positive_features.append(X_eval.columns[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot sorted feature importance\n",
    "indices = np.argsort(feature_importance)[::-1]  # Sort features by importance\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.bar(range(X_eval.shape[1]), feature_importance[indices])\n",
    "plt.xticks(range(X_eval.shape[1]), X_eval.columns[indices], rotation=90)\n",
    "#plt.tight_layout()\n",
    "plt.savefig(\"/work/SarahHvidAndersen#6681/DataScience_project/output/feature_exploration_plots/xgb_permuation_all_features.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF fitted\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth = None, min_samples_leaf=2,\n",
    "                                        min_samples_split = 2, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Define the F2 score as a custom scorer\n",
    "f2_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "# permutate\n",
    "result = permutation_importance(model, X_eval, y_eval, scoring=f2_scorer, n_repeats=10, random_state=42, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_features_rf = []\n",
    "\n",
    "# result.importances_mean provides the average decrease in model score due to the shuffling\n",
    "feature_importance = result.importances_mean\n",
    "for i in range(X_eval.shape[1]):\n",
    "    if feature_importance[i] > 0:\n",
    "        print(f\"Feature: {X_eval.columns[i]}, Importance: {feature_importance[i]}\")\n",
    "        positive_features_rf.append(X_eval.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(feature_importance)[::-1]  # Sort features by importance\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Feature Importances - Random Forest\")\n",
    "plt.bar(range(X_eval.shape[1]), feature_importance[indices])\n",
    "plt.xticks(range(X_eval.shape[1]), X_eval.columns[indices], rotation=90)\n",
    "plt.savefig(\"/work/SarahHvidAndersen#6681/DataScience_project/output/feature_exploration_plots/rf_permuation_all_features.png\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fit and compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save(model, X_train, y_train, filename):\n",
    "    \"\"\"Train a model and save it to a specified filename.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    dump(model, filename)\n",
    "    return model\n",
    "    \n",
    "def evaluate(model, X, y, model_name, feature_subset, beta=2):\n",
    "    probabilities = model.predict_proba(X)[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(y, probabilities)\n",
    "    \n",
    "    # Calculating F scores for the given beta\n",
    "    f_scores = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n",
    "    best_idx = np.nanargmax(f_scores)  # Using nanargmax to ignore NaNs in the calculation\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    predictions = (probabilities >= best_threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y, predictions)\n",
    "    prec = precision_score(y, predictions, zero_division=0)\n",
    "    rec = recall_score(y, predictions, zero_division=0)\n",
    "    f1 = f1_score(y, predictions, zero_division=0)\n",
    "    f2 = (1 + 2**2) * (prec * rec) / ((2**2 * prec + rec))\n",
    "    roc_auc = roc_auc_score(y, probabilities)\n",
    "\n",
    "    # Store performances\n",
    "    performances.append({\n",
    "        'model': model_name + '_' + str(len(feature_subset)),\n",
    "        'features': feature_subset,\n",
    "        'threshold': best_threshold,\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'f2': f2,\n",
    "        'roc_auc': roc_auc,\n",
    "        'beta': beta\n",
    "    })\n",
    "    \n",
    "    report = classification_report(y, predictions)\n",
    "    cm = confusion_matrix(y, predictions)\n",
    "\n",
    "    # Save the classification report as TXT\n",
    "    report_path = f'../models/feature_test/{model_name}_classification_report.txt'\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # Generate and save onfusion matrix as JPEG\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
    "    ax.set_title(f'Confusion Matrix for {model_name}')\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.set_xticklabels(['Negative', 'Positive'])\n",
    "    ax.set_yticklabels(['Negative', 'Positive'])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    matrix_path = f'../models/feature_test/{model_name}_confusion_matrix.jpeg'\n",
    "    plt.savefig(matrix_path)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved confusion matrix for {model_name} at {matrix_path}\")\n",
    "\n",
    "def test_feature_subsets(model, feature_subset, X_train, y_train, X_eval, y_eval, model_name):\n",
    "    X_train_subset = X_train[feature_subset]\n",
    "    X_eval_subset = X_eval[feature_subset]\n",
    "\n",
    "    filename = f\"../models/feature_test/{model_name}.joblib\"\n",
    "    trained_model = train_and_save(model, X_train_subset, y_train, filename) # fitting the subset\n",
    "    evaluate(trained_model, X_eval_subset, y_eval, f\"{model_name}\", feature_subset)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features with non-negative importance > 0.01\n",
    "#feature_subset = [feat for feat, imp in importance_scores.items() if imp > 0.01]\n",
    "\n",
    "all_features = ['Fwd IAT Total', 'Fwd Packet Length Max',\n",
    "        'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "        'Bwd Packets Length Total', 'Bwd Packet Length Max',\n",
    "        'Packet Length Max', 'Packet Length Mean', 'Packet Length Std',\n",
    "        'Packet Length Variance', 'Avg Packet Size', 'Fwd Header Length',\n",
    "        'Avg Fwd Segment Size', 'Avg Bwd Segment Size']\n",
    "        \n",
    "xgb_feature_subset_permutation = ['Fwd IAT Total', 'Fwd Packet Length Max', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    " 'Packet Length Std', 'Avg Packet Size', 'Fwd Header Length']\n",
    "xgb_feature_subset_rfecv = ['Fwd IAT Total', 'Fwd Packet Length Max', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    " 'Bwd Packets Length Total','Bwd Packet Length Max', 'Packet Length Max', 'Packet Length Mean',\n",
    " 'Packet Length Std', 'Packet Length Variance', 'Avg Packet Size', 'Fwd Header Length', 'Avg Fwd Segment Size']\n",
    "\n",
    "rf_feature_subset_rfecv =  ['Bwd Packet Length Std', 'Packet Length Variance', 'Avg Packet Size']\n",
    "rf_feature_subset_permutation = ['Bwd Packet Length Std', 'Fwd IAT Total', 'Fwd Header Length', 'Avg Packet Size']\n",
    "rf_feature_subset_perm_pos = ['Fwd IAT Total', 'Bwd Packet Length Std', 'Bwd Packets Length Total',\n",
    " 'Packet Length Mean','Avg Packet Size', 'Fwd Header Length', 'Avg Fwd Segment Size', 'Avg Bwd Segment Size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = plot_model_performance(perf_df, metric='f2') # rf14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performances = []\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = XGBClassifier(objective='binary:logistic', use_label_encoder=False,\n",
    "colsample_bytree= 1.0, gamma = 0.5, learning_rate= 0.3, max_depth= 10, n_estimators= 350, subsample= 0.5, eval_metric='logloss')\n",
    "\n",
    "test_feature_subsets(xgb_model, xgb_feature_subset_permutation, X_train, y_train, X_eval, y_eval, model_name = 'XGB_perm')\n",
    "test_feature_subsets(xgb_model, xgb_feature_subset_rfecv, X_train, y_train, X_eval, y_eval, model_name = 'XGB_rfecv')\n",
    "test_feature_subsets(xgb_model, all_features, X_train, y_train, X_eval, y_eval, model_name = 'XGB_all_features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest\n",
    "rf_model = RandomForestClassifier(n_estimators=110, max_depth = 40, min_samples_leaf=1,\n",
    "                                        min_samples_split = 10, random_state=42)\n",
    "\n",
    "test_feature_subsets(rf_model, rf_feature_subset_permutation, X_train, y_train, X_eval, y_eval, model_name = 'RF_perm')\n",
    "test_feature_subsets(rf_model, rf_feature_subset_rfecv, X_train, y_train, X_eval, y_eval, model_name = 'RF_rfecv')\n",
    "test_feature_subsets(rf_model, rf_feature_subset_perm_pos, X_train, y_train, X_eval, y_eval, model_name = 'RF_perm_pos')\n",
    "test_feature_subsets(rf_model, all_features, X_train, y_train, X_eval, y_eval, model_name = 'RF_all_features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "perf_df = pd.DataFrame(performances)\n",
    "perf_df.to_csv('../output/feature_reduction_results.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df = pd.read_csv('../output/feature_reduction_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(perf_df, metric='f2'):\n",
    "    #perf_df = pd.DataFrame(performances)\n",
    "    \n",
    "    # sort the full DataFrame based on the order of the test set performance\n",
    "    perf_df['model'] = pd.Categorical(perf_df['model'], ordered=True)\n",
    "    #perf_df = perf_df.sort_values('model')\n",
    "\n",
    "    # Set up the plot\n",
    "    sns.set_style('whitegrid')\n",
    "    scatter_plot = sns.scatterplot(data=perf_df, \n",
    "                                   x=metric, \n",
    "                                   y='model',\n",
    "                                   alpha=0.7)\n",
    "\n",
    "    scatter_plot.set_title('Model Performance Comparison')\n",
    "    scatter_plot.set_xlabel(f'{metric.capitalize()} Score')\n",
    "    scatter_plot.set_ylabel('Models')\n",
    "    plt.show()\n",
    "    return perf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
