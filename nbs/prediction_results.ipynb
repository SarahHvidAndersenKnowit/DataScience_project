{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import dump\n",
    "import pickle as pkl\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer, fbeta_score,  mean_squared_error, r2_score, f1_score\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, precision_recall_curve, classification_report, confusion_matrix)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, GridSearchCV, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/work/SarahHvidAndersen#6681/DataScience_project/data/train_df.csv')\n",
    "eval_df = pd.read_csv('/work/SarahHvidAndersen#6681/DataScience_project/data/eval_df.csv')\n",
    "test_df = pd.read_csv('/work/SarahHvidAndersen#6681/DataScience_project/data/test_df.csv')\n",
    "\n",
    "\"\"\"\n",
    "selected_features = ['Fwd IAT Total', 'Fwd Packet Length Max',\n",
    "        'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "        'Bwd Packets Length Total', 'Bwd Packet Length Max',\n",
    "        'Packet Length Max', 'Packet Length Mean', 'Packet Length Std',\n",
    "        'Packet Length Variance', 'Avg Packet Size', 'Fwd Header Length',\n",
    "        'Avg Fwd Segment Size', 'Avg Bwd Segment Size']\n",
    "\"\"\"\n",
    "\n",
    "#X_train = train_df[selected_features]  # Features from the training data\n",
    "X_train = train_df.drop(['Label', 'Attack'], axis=1)\n",
    "y_train = train_df['Attack'] \n",
    "\n",
    "#X_eval = eval_df[selected_features]  # Features from the evaluation data\n",
    "X_eval = eval_df.drop(['Label', 'Attack'], axis=1)\n",
    "y_eval = eval_df['Attack']\n",
    "\n",
    "#X_test = test_df[selected_features]\n",
    "X_test = test_df.drop(['Label', 'Attack'], axis=1)\n",
    "y_test = test_df['Attack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the features\n",
    "X_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])\n",
    "\n",
    "# Applying the 'trained' scaler on eval and test\n",
    "X_eval[X_eval.columns] = scaler.transform(X_eval[X_eval.columns])\n",
    "X_test[X_test.columns] = scaler.transform(X_test[X_test.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/work/SarahHvidAndersen#6681/DataScience_project/output/test_df_with_predictions.csv')\n",
    "relevant_columns = [ 'Label', 'Attack', 'Baseline_RF_Absolute_Features_Predictions', 'Baseline_XGB_Absolute_Features_Predictions',\n",
    "                              'Baseline_RF_All_Features_Predictions','Tuned_RF_All_Features_Predictions','Tuned_RF_Permutation_Features_Predictions',\n",
    "                            'Baseline_XGB_All_Features_Predictions','Tuned_XGB_All_Features_Predictions','Tuned_XGB_Permutation_Features_Predictions','Tuned_XGB_RFECV_Features_Predictions']\n",
    "\n",
    "df = df[relevant_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all model predictions are '0' (no attack predicted)\n",
    "actual_attacks = df[df['Attack'] == 1]\n",
    "prediction_columns = [col for col in df.columns if col.startswith('Baseline_') or col.startswith('Tuned_')]\n",
    "\n",
    "failed_predictions = actual_attacks[(actual_attacks[prediction_columns] == 0).all(axis=1)]\n",
    "failed_predictions_with_labels = failed_predictions[['Label'] + prediction_columns]\n",
    "\n",
    "print(f\"Number of cases where all models failed to predict an attack: {failed_predictions_with_labels.shape[0]}\")\n",
    "# Number of cases where all models failed to predict an attack: 52\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check where best recall and f2 model predictions are '0' (no attack predicted)\n",
    "actual_attacks = df[df['Attack'] == 1]\n",
    "prediction_columns = [col for col in df.columns if col.startswith('Baseline_RF_A') or col.startswith('Tuned_RF_A')]\n",
    "\n",
    "failed_predictions = actual_attacks[(actual_attacks[prediction_columns] == 0).all(axis=1)]\n",
    "failed_predictions_with_labels = failed_predictions[['Label'] + prediction_columns]\n",
    "\n",
    "print(f\"Number of cases where best RF models failed to predict an attack: {failed_predictions_with_labels.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify model prediction columns\n",
    "all_model_columns = [col for col in df.columns if col.startswith('Baseline_') or col.startswith('Tuned_')]\n",
    "rf_model_columns = [col for col in all_model_columns if '_RF_' in col]\n",
    "xgb_model_columns = [col for col in all_model_columns if '_XGB_' in col]\n",
    "\n",
    "# Calculate failures for each category\n",
    "df['Total_Model_Failures'] = df[all_model_columns].apply(lambda x: (x == 0).sum(), axis=1)\n",
    "df['RF_Model_Failures'] = df[rf_model_columns].apply(lambda x: (x == 0).sum(), axis=1)\n",
    "df['XGB_Model_Failures'] = df[xgb_model_columns].apply(lambda x: (x == 0).sum(), axis=1)\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create heatmap for failed predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model columns and filter attacks\n",
    "model_columns = [col for col in df.columns if col.startswith('Baseline_') or col.startswith('Tuned_')]\n",
    "actual_attacks = df[df['Attack'] == 1]\n",
    "\n",
    "# Further filter to find rows where any model failed to predict the attack\n",
    "failed_attacks_mask = actual_attacks[model_columns].apply(lambda x: (x == 0).any(), axis=1)\n",
    "failed_attacks = actual_attacks[failed_attacks_mask]\n",
    "\n",
    "# Create a binary DataFrame indicating failures (1) and successes (0)\n",
    "failure_binary_df = failed_attacks[model_columns].applymap(lambda x: 1 if x == 0 else 0)\n",
    "\n",
    "# Include the 'Label' column in the failure_binary_df for grouping\n",
    "failure_binary_df['Label'] = failed_attacks['Label']\n",
    "\n",
    "# Group by 'Label' and sort groups by their size\n",
    "label_counts = failure_binary_df['Label'].value_counts()\n",
    "#sorted_labels = label_counts.sort_values(ascending=True).index\n",
    "sorted_labels = np.sort(failure_binary_df['Label'].unique())\n",
    "\n",
    "# Sort the DataFrame by this order\n",
    "failure_binary_df['Label'] = pd.Categorical(failure_binary_df['Label'], categories=sorted_labels, ordered=True)\n",
    "sorted_failure_df = failure_binary_df.sort_values(by='Label')\n",
    "\n",
    "# Separate the label for plotting and remove it from data for heatmap\n",
    "labels = sorted_failure_df['Label']\n",
    "sorted_failure_df = sorted_failure_df.drop(columns='Label')\n",
    "\n",
    "# Transpose for heatmap plotting\n",
    "sorted_failure_df_transposed = sorted_failure_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot heatmap\n",
    "plt.figure(figsize=(15, 4))\n",
    "cmap = mcolors.ListedColormap(['#7fc97f', '#f94e42'])\n",
    "ax = sns.heatmap(sorted_failure_df_transposed, cmap=cmap, cbar=False, linewidths=0.5, linecolor='black')\n",
    "\n",
    "# Add vertical lines between groups\n",
    "cumulative_counts = np.cumsum(label_counts[sorted_labels])\n",
    "for count in cumulative_counts[:-1]:  # Skip the last line\n",
    "    plt.axvline(x=count, color='blue', linestyle='--', linewidth=2.6)  # Change color and style as needed\n",
    "\n",
    "# Setup x-ticks for attack types\n",
    "tick_positions = cumulative_counts - label_counts[sorted_labels] / 2\n",
    "plt.xticks(tick_positions, sorted_labels, rotation=45, ha='right')\n",
    "plt.yticks(fontsize = 13)\n",
    "\n",
    "ax.set_title('Heatmap of Model Failures Grouped by Attack Type', size = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get columns\n",
    "model_columns = [col for col in df.columns if col.startswith('Baseline_') or col.startswith('Tuned_')]\n",
    "\n",
    "# Create a binary DataFrame where 1 indicates a failure and 0 indicates a success\n",
    "attack_cases = df[df['Attack'] == 1]\n",
    "\n",
    "# For each model, mark failure (where prediction is 0) and success (where prediction is 1)\n",
    "failure_binary_df = attack_cases[model_columns].applymap(lambda x: 1 if x == 0 else 0)\n",
    "failure_binary_df['Label'] = attack_cases['Label']\n",
    "\n",
    "# Group by 'Label' and calculate the mean of each group for each model\n",
    "failure_rates = failure_binary_df.groupby('Label').mean()\n",
    "failure_rates = failure_rates.T\n",
    "\n",
    "# multiply by 100 to convert proportions to percentages\n",
    "#failure_rates *= 100\n",
    "failure_rates = failure_rates.round(2)\n",
    "failure_rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create heatmap of proportion of failures\n",
    "plt.figure(figsize=(9, 5))\n",
    "ax = sns.heatmap(failure_rates, annot=True, cmap='Reds', linewidths=.5, cbar_kws={'label': 'Failure Rate (%)'})\n",
    "ax.set_title('Heatmap of Model Failure Rates by Attack Type')\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.xlabel('')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the counts of each label in the train, eval, and test datasets\n",
    "train_counts = train_df['Label'].value_counts().rename('Train Count')\n",
    "eval_counts = eval_df['Label'].value_counts().rename('Eval Count')\n",
    "test_counts = test_df['Label'].value_counts().rename('Test Count')\n",
    "\n",
    "# Combine counts into a single DataFrame\n",
    "counts_df = pd.DataFrame([train_counts, eval_counts, test_counts]).T\n",
    "counts_df.fillna(0, inplace=True)  # Replace NaNs with 0 where labels might not appear in one of the dataframes\n",
    "failure_rates_transposed = failure_rates.T  # Transpose to have labels as rows and models as columns\n",
    "\n",
    "failed_predictions_count = failure_binary_df.drop(columns='Label').apply(sum, axis=1)  # Sum failures across all models for each instance\n",
    "failed_predictions_count = failure_binary_df.groupby('Label').apply(lambda df: df.drop(columns='Label').values.sum()).rename('Failed Predictions')\n",
    "\n",
    "# Calculate the sum of failed predictions for each model within each label\n",
    "model_failed_counts = failure_binary_df.groupby('Label')[model_columns].sum().add_suffix('_Failures')\n",
    "\n",
    "# merging\n",
    "final_stats = pd.concat([counts_df, failure_rates_transposed, model_failed_counts], axis=1)\n",
    "final_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_stats.to_csv('/work/SarahHvidAndersen#6681/DataScience_project/output/count_statistics.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
