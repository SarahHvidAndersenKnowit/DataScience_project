{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import dump\n",
    "import pickle as pkl\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "\n",
    "from sklearn.metrics import make_scorer, fbeta_score,  mean_squared_error, r2_score, f1_score\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, precision_recall_curve, classification_report, confusion_matrix)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, GridSearchCV, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/work/SarahHvidAndersen#6681/DataScience_project/data/train_df.csv')\n",
    "eval_df = pd.read_csv('/work/SarahHvidAndersen#6681/DataScience_project/data/eval_df.csv')\n",
    "test_df = pd.read_csv('/work/SarahHvidAndersen#6681/DataScience_project/data/test_df.csv')\n",
    "\n",
    "\"\"\"\n",
    "selected_features = ['Fwd IAT Total', 'Fwd Packet Length Max',\n",
    "        'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "        'Bwd Packets Length Total', 'Bwd Packet Length Max',\n",
    "        'Packet Length Max', 'Packet Length Mean', 'Packet Length Std',\n",
    "        'Packet Length Variance', 'Avg Packet Size', 'Fwd Header Length',\n",
    "        'Avg Fwd Segment Size', 'Avg Bwd Segment Size']\n",
    "\"\"\"\n",
    "\n",
    "#X_train = train_df[selected_features]  # Features from the training data\n",
    "X_train = train_df.drop(['Label', 'Attack'], axis=1)\n",
    "y_train = train_df['Attack'] \n",
    "\n",
    "#X_eval = eval_df[selected_features]  # Features from the evaluation data\n",
    "X_eval = eval_df.drop(['Label', 'Attack'], axis=1)\n",
    "y_eval = eval_df['Attack']\n",
    "\n",
    "#X_test = test_df[selected_features]\n",
    "X_test = test_df.drop(['Label', 'Attack'], axis=1)\n",
    "y_test = test_df['Attack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the features\n",
    "X_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])\n",
    "\n",
    "# Applying the 'trained' scaler on eval and test\n",
    "X_eval[X_eval.columns] = scaler.transform(X_eval[X_eval.columns])\n",
    "X_test[X_test.columns] = scaler.transform(X_test[X_test.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save(model, X_train, y_train, filename):\n",
    "    \"\"\"Train a model and save it to a specified filename.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    dump(model, filename)\n",
    "    return model\n",
    "\n",
    "def optimize_threshold(model, X, y, beta=2):\n",
    "    probabilities = model.predict_proba(X)[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(y, probabilities)\n",
    "    f_scores = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n",
    "    best_idx = np.nanargmax(f_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    return best_threshold\n",
    "\n",
    "def evaluate_and_save(model, y, predictions, probabilities, best_threshold, model_name, feature_subset):\n",
    "    acc = accuracy_score(y, predictions)\n",
    "    prec = precision_score(y, predictions, zero_division=0)\n",
    "    rec = recall_score(y, predictions, zero_division=0)\n",
    "    f1 = f1_score(y, predictions, zero_division=0)\n",
    "    f2 = (1 + 2**2) * (prec * rec) / ((2**2 * prec + rec))\n",
    "    roc_auc = roc_auc_score(y, probabilities)\n",
    "\n",
    "    # appending results\n",
    "    performances.append({\n",
    "        'model': model_name,\n",
    "        'features': feature_subset,\n",
    "        'threshold': best_threshold, 'accuracy': acc,'precision': prec,'recall': rec,\n",
    "        'f1': f1,'f2': f2,'roc_auc': roc_auc})\n",
    "    \n",
    "    report = classification_report(y, predictions)\n",
    "    cm = confusion_matrix(y, predictions)\n",
    "\n",
    "    # Save the classification report as TXT\n",
    "    report_path = f'../output/best_model_evaluation/{model_name}_classification_report.txt'\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # Generate and save onfusion matrix as JPEG\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
    "    ax.set_title(f'Confusion Matrix for {model_name}')\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.set_xticklabels(['Negative', 'Positive'])\n",
    "    ax.set_yticklabels(['Negative', 'Positive'])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    matrix_path = f'../output/best_model_evaluation/{model_name}_confusion_matrix.jpeg'\n",
    "    plt.savefig(matrix_path)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved results for {model_name} at {matrix_path}\")\n",
    "    return\n",
    "\n",
    "def process_model(model, feature_subset, X_train, y_train, X_val, y_val, X_test, y_test, model_name):\n",
    "    # Subset the features for the different datasets\n",
    "    X_train_subset = X_train[feature_subset]\n",
    "    X_val_subset = X_val[feature_subset]\n",
    "    X_test_subset = X_test[feature_subset]\n",
    "\n",
    "    # Train the model\n",
    "    filename = f\"../models/best_models/{model_name}_model.joblib\"\n",
    "    trained_model = train_and_save(model, X_train_subset, y_train, filename)\n",
    "\n",
    "    # Optimize threshold on validation set\n",
    "    best_threshold = optimize_threshold(trained_model, X_val_subset, y_val)\n",
    "\n",
    "    # Apply model to the test set using the optimized threshold\n",
    "    probabilities = trained_model.predict_proba(X_test_subset)[:, 1]\n",
    "    predictions = (probabilities >= best_threshold).astype(int)\n",
    "\n",
    "    # create predictions and save results\n",
    "    evaluate_and_save(trained_model, y_test, predictions, probabilities, best_threshold, model_name, feature_subset)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Baseline_RF': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "\n",
    "    'Tuned_RF': RandomForestClassifier(n_estimators=110, max_depth = 40, min_samples_leaf=1,\n",
    "                                        min_samples_split = 10, random_state=42),\n",
    "    'Tuned_RFall': RandomForestClassifier(n_estimators=100, max_depth = None, min_samples_leaf=2,\n",
    "                                        min_samples_split = 2, random_state=42),\n",
    "\n",
    "    'Baseline_XGB': XGBClassifier(objective='binary:logistic', use_label_encoder=False),\n",
    "\n",
    "    'Tuned_XGB': XGBClassifier(objective='binary:logistic',use_label_encoder=False,\n",
    "                                colsample_bytree=1.0, gamma=0.5,learning_rate=0.3, max_depth=10,\n",
    "                                n_estimators=350, subsample=0.5, eval_metric='logloss'),\n",
    "                                \n",
    "    'Tuned_XGBall': XGBClassifier(objective='binary:logistic',use_label_encoder=False,\n",
    "                                colsample_bytree=1.0, gamma=1,learning_rate=0.3, max_depth=3,\n",
    "                                n_estimators=300, subsample=0.7, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "selected_features = ['Fwd IAT Total', 'Fwd Packet Length Max', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "                              'Bwd Packets Length Total', 'Bwd Packet Length Max', 'Packet Length Max', 'Packet Length Mean',\n",
    "                              'Packet Length Std', 'Packet Length Variance', 'Avg Packet Size', 'Fwd Header Length',\n",
    "                              'Avg Fwd Segment Size', 'Avg Bwd Segment Size']\n",
    "\n",
    "all_features = [col for col in X_train.columns]\n",
    "\n",
    "models_config = {\n",
    "    'Baseline_RF_All_Features': all_features,\n",
    "    'Tuned_RFall_Features': all_features,\n",
    "    'Baseline_RF_selected_Features': selected_features,\n",
    "    'Tuned_RF_selected_Features': selected_features,\n",
    "    'Tuned_RF_Permutation_Features': ['Fwd IAT Total', 'Bwd Packet Length Std', 'Bwd Packets Length Total',\n",
    "                                   'Packet Length Mean', 'Avg Packet Size', 'Fwd Header Length', 'Avg Fwd Segment Size',\n",
    "                                   'Avg Bwd Segment Size'],\n",
    "\n",
    "    'Baseline_XGB_All_Features': all_features,\n",
    "    'Tuned_XGBall_Features': all_features,                          \n",
    "    'Baseline_XGB_selected_Features': selected_features,\n",
    "    'Tuned_XGB_selected_Features': selected_features,\n",
    "    'Tuned_XGB_Permutation_Features': ['Fwd IAT Total', 'Fwd Packet Length Max', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "                                       'Packet Length Std', 'Avg Packet Size', 'Fwd Header Length'],\n",
    "    'Tuned_XGB_RFECV_Features': ['Fwd IAT Total', 'Fwd Packet Length Max', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "                                 'Bwd Packets Length Total', 'Bwd Packet Length Max', 'Packet Length Max', 'Packet Length Mean',\n",
    "                                 'Packet Length Std', 'Packet Length Variance', 'Avg Packet Size', 'Fwd Header Length', 'Avg Fwd Segment Size']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fror validation results, with optimized threshold\n",
    "all_predictions = test_df\n",
    "performances = []\n",
    "\n",
    "# Iterating over configurations\n",
    "for model_name, features in models_config.items():\n",
    "    # Extract the base model name (before the underscore)\n",
    "    base_model_name = '_'.join(model_name.split('_')[:2])\n",
    "    print(f'\\n running {model_name} with {features} and base model {base_model_name}')                  # using eval here\n",
    "    predictions = process_model(models[base_model_name], features, X_train, y_train, X_eval, y_eval, X_eval, y_eval, model_name)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_results_df = pd.DataFrame(performances)\n",
    "val_results_df['split'] = 'Val'\n",
    "val_results_df                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for test results, with threshold from val set\n",
    "all_predictions = test_df\n",
    "performances = []\n",
    "\n",
    "# Iterating over configurations\n",
    "for model_name, features in models_config.items():\n",
    "    # Extract the base model name (before the underscore)\n",
    "    base_model_name = '_'.join(model_name.split('_')[:2])\n",
    "    print(f'\\n running {model_name} with {features} and base model {base_model_name}')                  # back to test\n",
    "    predictions = process_model(models[base_model_name], features, X_train, y_train, X_eval, y_eval, X_test, y_test, model_name)\n",
    "    # Add predictions to the DataFrame\n",
    "    all_predictions[f'{model_name}_Predictions'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions.to_csv('/work/SarahHvidAndersen#6681/DataScience_project/output/test_df_with_predictions.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results_df = pd.DataFrame(performances)\n",
    "test_results_df['split'] = 'Test'\n",
    "test_results_df                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([test_results_df, val_results_df])\n",
    "df.to_csv('/work/SarahHvidAndersen#6681/DataScience_project/output/val_test_performances.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_best_model_splits(perf_df, metric='f2'):\n",
    "    # Filter DataFrame for 'Test' and sort based on the metric\n",
    "    test_perf = perf_df[perf_df['split'] == 'Test'].sort_values(by=metric, ascending=False)\n",
    "    \n",
    "    # Creating a list of models sorted by their test performance without duplicates\n",
    "    sorted_models = test_perf['model'].drop_duplicates().tolist()\n",
    "    perf_df['model_order'] = pd.Categorical(perf_df['model'], categories=sorted_models, ordered=True)\n",
    "\n",
    "    # Set up the plot\n",
    "    sns.set_style('whitegrid')\n",
    "    scatter_plot = sns.scatterplot(data=perf_df, \n",
    "                                   y='model_order', \n",
    "                                   x=metric, \n",
    "                                   hue='split',\n",
    "                                   style='split', \n",
    "                                   markers={       # Define markers for each split\n",
    "                                       'Test': 'o',\n",
    "                                       'Val': '^',\n",
    "                                   },\n",
    "                                   palette=['darkred', 'darkorange'],\n",
    "                                   alpha=0.8)  # Adjust alpha transparency for all markers\n",
    "\n",
    "    scatter_plot.set_title('Model Performance Comparison')\n",
    "    scatter_plot.set_xlabel(f'{metric.capitalize()} Score')\n",
    "    scatter_plot.set_ylabel('Models')\n",
    "    # Specify the plot path and save\n",
    "    plt.tight_layout() \n",
    "    plot_path = f'../output/plots/{metric}_bestmodel_performance_splits.jpeg'\n",
    "    plt.savefig(plot_path)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ['f1', 'f2', 'recall', 'precision', 'roc_auc', 'accuracy']:\n",
    "    plot_best_model_splits(df, metric = m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_best_model_splits(full_results, metric = 'recall')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
