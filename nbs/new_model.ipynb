{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from joblib import dump\n",
    "import pickle as pkl\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, precision_recall_curve\n",
    "from sklearn.metrics import make_scorer, fbeta_score,  mean_squared_error, r2_score, f1_score\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             roc_auc_score, precision_recall_curve, classification_report, confusion_matrix)\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, GridSearchCV, train_test_split, StratifiedShuffleSplit\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, plot_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recover; initial model generation and plot comparison, robertas code\n",
    "running the final models, with eval threshold\n",
    "compare test results in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/work/SarahHvidAndersen#6681/DataScience_project/data/train_df.csv')\n",
    "eval_df = pd.read_csv('/work/SarahHvidAndersen#6681/DataScience_project/data/eval_df.csv')\n",
    "test_df = pd.read_csv('/work/SarahHvidAndersen#6681/DataScience_project/data/test_df.csv')\n",
    "\n",
    "selected_features = ['Fwd IAT Total', 'Fwd Packet Length Max',\n",
    "        'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "        'Bwd Packets Length Total', 'Bwd Packet Length Max',\n",
    "        'Packet Length Max', 'Packet Length Mean', 'Packet Length Std',\n",
    "        'Packet Length Variance', 'Avg Packet Size', 'Fwd Header Length',\n",
    "        'Avg Fwd Segment Size', 'Avg Bwd Segment Size']\n",
    "\n",
    "X_train = train_df[selected_features]  # Features from the training data\n",
    "y_train = train_df['Attack'] \n",
    "\n",
    "X_eval = eval_df[selected_features]  # Features from the evaluation data\n",
    "y_eval = eval_df['Attack']\n",
    "\n",
    "X_test = test_df[selected_features]\n",
    "y_test = test_df['Attack']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the features\n",
    "X_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])\n",
    "\n",
    "# Applying the 'trained' scaler on eval and test\n",
    "X_eval[X_eval.columns] = scaler.transform(X_eval[X_eval.columns])\n",
    "X_test[X_test.columns] = scaler.transform(X_test[X_test.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", zero_division=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, f1_score, accuracy_score, recall_score, precision_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate(model, X, y, nsplit, model_name, beta=2):\n",
    "    probabilities = model.predict_proba(X)[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(y, probabilities)\n",
    "    \n",
    "    # Calculating F scores for the given beta\n",
    "    f_scores = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n",
    "    best_idx = np.nanargmax(f_scores)  # Using nanargmax to ignore NaNs in the calculation\n",
    "    best_threshold = thresholds[best_idx]\n",
    "\n",
    "    predictions = (probabilities >= best_threshold).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y, predictions)\n",
    "    prec = precision_score(y, predictions, zero_division=0)  # Handle zero division case\n",
    "    rec = recall_score(y, predictions)\n",
    "    f1 = f1_score(y, predictions)\n",
    "    f2 = (1 + 2**2) * (prec * rec) / ((2**2 * prec) + rec)\n",
    "    roc_auc = roc_auc_score(y, probabilities) \n",
    "\n",
    "    # Store performances\n",
    "   performances.append({\n",
    "        'model': model_name,\n",
    "        'split': nsplit,\n",
    "        'threshold': best_threshold,\n",
    "        'accuracy': acc,'precision': prec,'recall': rec,'f1': f1,\n",
    "        'f2': f2,\n",
    "        # add f2\n",
    "        'roc_auc': roc_auc,\n",
    "        'beta': beta\n",
    "    })\n",
    "def run_on_splits(func):\n",
    "    def _run_loop(model, model_name):\n",
    "        for X, y, nsplit in zip([X_train, X_val, X_test], [y_train, y_val, y_test], ['train', 'val', 'test']):\n",
    "            func(model, X=X, y=y, nsplit=nsplit, model_name=model_name, beta=1)  # F1 score optimization\n",
    "            func(model, X=X, y=y, nsplit=nsplit, model_name=model_name, beta=2)  # F2 score optimization\n",
    "    return _run_loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@run_on_splits\n",
    "def evaluate(model, X, y, nsplit, model_name, beta=1):\n",
    "    # Function body remains the same\n",
    "    ...\n",
    "\n",
    "# Example usage with a hypothetical model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train, y_train)  # Assuming model is fitted\n",
    "\n",
    "# Evaluate with the decorator handling train, validation, and test splits\n",
    "evaluate(model, model_name='RandomForest')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot with "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "def train_and_save(model, X_train, y_train, filename):\n",
    "    \"\"\"Train a model and save it.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    dump(model, filename)\n",
    "    return model\n",
    "\n",
    "# Assuming X_train and y_train are defined\n",
    "models = {\n",
    "    'Dummy_Classifier': DummyClassifier(strategy='most_frequent', random_state=42),\n",
    "    'Baseline_log_reg': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Baseline_SVM': svm.SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'Baseline_KNN': KNeighborsClassifier(n_neighbors=5, random_state=42),\n",
    "    'Baseline_RF': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Tuned_RF': RandomForestClassifier(n_estimators=110, max_depth=40, min_samples_leaf=1, min_samples_split=10, random_state=42),\n",
    "    'Baseline_XGB': XGBClassifier(objective='binary:logistic', use_label_encoder=False, eval_metric='logloss'),\n",
    "    'Tuned_XGB': XGBClassifier(objective='binary:logistic', use_label_encoder=False, colsample_bytree=1.0, gamma=0.5, learning_rate=0.3, max_depth=10, n_estimators=350, subsample=0.5, eval_metric='logloss')\n",
    "}\n",
    "\n",
    "# Train and save each model\n",
    "for name, model in models.items():\n",
    "    train_and_save(model, X_train, y_train, f'../models/{name}.joblib')\n",
    "\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "def train_and_save(model, X_train, y_train, filename):\n",
    "    \"\"\"Train a model and save it.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    dump(model, filename)\n",
    "    return model\n",
    "\n",
    "models = {\n",
    "    # Define your models here as shown previously\n",
    "}\n",
    "\n",
    "trained_models = {}  # Dictionary to store trained model references\n",
    "\n",
    "for name, model in models.items():\n",
    "    trained_model = train_and_save(model, X_train, y_train, f'../models/{name}.joblib')\n",
    "    trained_models[name] = trained_model  # Store the trained model for later use\n",
    "\n",
    "performances = []  # To store performance results\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    evaluate(model, model_name=name)\n",
    "\n",
    "from joblib import load\n",
    "\n",
    "def load_and_evaluate(model_filename, model_name):\n",
    "    model = load(model_filename)\n",
    "    evaluate(model, model_name=model_name)\n",
    "\n",
    "# Assuming you know the model names and have them stored or hardcoded\n",
    "model_names = ['RandomForest', 'SVM', 'LogisticRegression']  # etc.\n",
    "for name in model_names:\n",
    "    load_and_evaluate(f'../models/{name}.joblib', name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_model_performance(performances, metric='f1'):\n",
    "    # Create DataFrame from performances\n",
    "    perf_df = pd.DataFrame(performances)\n",
    "\n",
    "    # Filter to get only the test set performances\n",
    "    test_perf = perf_df[perf_df['split'] == 'test'].sort_values(by=metric, ascending=False)\n",
    "\n",
    "    # Now, sort the full DataFrame based on the order of the test set performance\n",
    "    perf_df['model'] = pd.Categorical(perf_df['model'], categories=test_perf['model'], ordered=True)\n",
    "    perf_df = perf_df.sort_values('model')\n",
    "\n",
    "    # Set up the plot\n",
    "    sns.set_style('whitegrid')\n",
    "    scatter_plot = sns.scatterplot(data=perf_df, \n",
    "                                   y='model', \n",
    "                                   x=metric, \n",
    "                                   hue='split',\n",
    "                                   style='split',  # Ensure this is used\n",
    "                                   markers={       # Define markers for each split\n",
    "                                       'train': 'o',  # Circle marker for training set\n",
    "                                       'val': '^',    # Triangle marker for validation set\n",
    "                                       'test': 's'    # Square marker for test set\n",
    "                                   },\n",
    "                                   palette=['grey', 'darkorange', 'darkred'],\n",
    "                                   alpha=0.8)  # Adjust alpha transparency for all markers\n",
    "\n",
    "    scatter_plot.set_title('Model Performance Comparison')\n",
    "    scatter_plot.set_xlabel(f'{metric.capitalize()} Score')\n",
    "    scatter_plot.set_ylabel('Models')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance(perf_df, metric='f2'):\n",
    "    #perf_df = pd.DataFrame(performances)\n",
    "    \n",
    "    # sort the full DataFrame based on the order of the test set performance\n",
    "    perf_df['model'] = pd.Categorical(perf_df['model'], ordered=True)\n",
    "    #perf_df = perf_df.sort_values('model')\n",
    "\n",
    "    # Set up the plot\n",
    "    sns.set_style('whitegrid')\n",
    "    scatter_plot = sns.scatterplot(data=perf_df, \n",
    "                                   x=metric, \n",
    "                                   y='model',\n",
    "                                   alpha=0.7)\n",
    "\n",
    "    scatter_plot.set_title('Model Performance Comparison')\n",
    "    scatter_plot.set_xlabel(f'{metric.capitalize()} Score')\n",
    "    scatter_plot.set_ylabel('Models')\n",
    "    plt.show()\n",
    "    return perf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best models from baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create DataFrame from performances\n",
    "perf_df = pd.DataFrame(performances)\n",
    "\n",
    "# Filter for test split and sort by F2 score\n",
    "top_models = perf_df[perf_df['split'] == 'test'].sort_values(by='f2', ascending=False).head(8)\n",
    "\n",
    "def evaluate_and_save_results(model, X, y, model_name, filepath):\n",
    "    predictions = model.predict(X)\n",
    "    report = classification_report(y, predictions)\n",
    "    cm = confusion_matrix(y, predictions)\n",
    "\n",
    "    \n",
    "    if nsplit == 'val':\n",
    "        report = classification_report(y, predictions)\n",
    "        cm = confusion_matrix(y, predictions)\n",
    "\n",
    "        # Save the classification report as TXT\n",
    "        report_path = f'../output/{model_name}_classification_report.txt'\n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(report)\n",
    "\n",
    "        # Generate and save onfusion matrix as JPEG\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
    "        ax.set_title(f'Confusion Matrix for {model_name}')\n",
    "        ax.set_xlabel('Predicted Labels')\n",
    "        ax.set_ylabel('True Labels')\n",
    "        ax.set_xticklabels(['Negative', 'Positive'])\n",
    "        ax.set_yticklabels(['Negative', 'Positive'])\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        matrix_path = f'../output/{model_name}_confusion_matrix.jpeg'\n",
    "        plt.savefig(matrix_path)\n",
    "        plt.close(fig)\n",
    "        print(f\"Saved confusion matrix for {model_name} at {matrix_path}\")\n",
    "    )\n",
    "\n",
    "# Assuming model is already loaded or defined\n",
    "# X_test, y_test should be defined as your test dataset\n",
    "evaluate_and_save_results(model, X_test, y_test, 'RandomForest', '../model_outputs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Assuming the loop setup is correct and 'top_models' DataFrame is prepared as shown\n",
    "for _, row in top_models.iterrows():\n",
    "    model_name = row['model']\n",
    "    # Construct paths based on how files were saved\n",
    "    report_path = f'../output/{model_name}_classification_report.txt'\n",
    "    matrix_path = f'../output/{model_name}_confusion_matrix.jpeg'\n",
    "\n",
    "    # Load and print the classification report\n",
    "    try:\n",
    "        with open(report_path, 'r') as file:\n",
    "            classification_report = file.read()\n",
    "        print(f\"Classification Report for {model_name}:\\n{classification_report}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Classification report for {model_name} not found at {report_path}\")\n",
    "\n",
    "    # Display the confusion matrix image\n",
    "    try:\n",
    "        img = mpimg.imread(matrix_path)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Turn off axis numbers and ticks\n",
    "        plt.title(f\"Confusion Matrix for {model_name}\")\n",
    "        plt.show()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Confusion matrix for {model_name} not found at {matrix_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_and_save(model, X_train, y_train, filename):\n",
    "    \"\"\"Train a model and save it to a specified filename.\"\"\"\n",
    "    model.fit(X_train, y_train)\n",
    "    dump(model, filename)\n",
    "    return model\n",
    "\n",
    "def optimize_threshold(model, X, y, beta=2):\n",
    "    probabilities = model.predict_proba(X)[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(y, probabilities)\n",
    "    f_scores = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n",
    "    best_idx = np.nanargmax(f_scores)\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    return best_threshold\n",
    "\n",
    "def evaluate(probabilities, predictions, best_threshold):\n",
    "\n",
    "    performances.append({\n",
    "        'model': model_name,\n",
    "        'features': feature_subset,\n",
    "        'threshold': best_threshold, 'accuracy': acc,'precision': prec,'recall': rec,\n",
    "        'f1': f1,'f2': f2,'roc_auc': roc_auc})\n",
    "    \n",
    "    report = classification_report(y, predictions)\n",
    "    cm = confusion_matrix(y, predictions)\n",
    "\n",
    "    # Save the classification report as TXT\n",
    "    report_path = f'../models/feature_test/{model_name}_classification_report.txt'\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    # Generate and save onfusion matrix as JPEG\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax, cbar=False)\n",
    "    ax.set_title(f'Confusion Matrix for {model_name}')\n",
    "    ax.set_xlabel('Predicted Labels')\n",
    "    ax.set_ylabel('True Labels')\n",
    "    ax.set_xticklabels(['Negative', 'Positive'])\n",
    "    ax.set_yticklabels(['Negative', 'Positive'])\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    matrix_path = f'../models/feature_test/{model_name}_confusion_matrix.jpeg'\n",
    "    plt.savefig(matrix_path)\n",
    "    plt.close(fig)\n",
    "    print(f\"Saved confusion matrix for {model_name} at {matrix_path}\")\n",
    "    return\n",
    "\n",
    "def process_model(model, feature_subset, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "    # Subset the features for the different datasets\n",
    "    X_train_subset = X_train[feature_subset]\n",
    "    X_val_subset = X_val[feature_subset]\n",
    "    X_test_subset = X_test[feature_subset]\n",
    "\n",
    "    # Train the model\n",
    "    filename = f\"../models/{model_name}_model.joblib\"\n",
    "    trained_model = train_and_save(model, X_train_subset, y_train, filename)\n",
    "\n",
    "    # Optimize threshold on validation set\n",
    "    best_threshold = optimize_\n",
    "    threshold(trained_model, X_val_subset, y_val)\n",
    "\n",
    "    # Apply model to the test set using the optimized threshold\n",
    "    probabilities = trained_model.predict_proba(X_test_subset)[:, 1]\n",
    "    predictions = (probabilities >= best_threshold).astype(int)\n",
    "\n",
    "    evaluate()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'Baseline_RF': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "\n",
    "    'Tuned_RF': RandomForestClassifier(n_estimators=110, max_depth = 40, min_samples_leaf=1,\n",
    "                                        min_samples_split = 10, random_state=42),\n",
    "\n",
    "    'Baseline_XGB': XGBClassifier(objective='binary:logistic', use_label_encoder=False),\n",
    "\n",
    "    'Tuned_XGB': XGBClassifier(objective='binary:logistic',use_label_encoder=False,\n",
    "                                colsample_bytree=1.0, gamma=0.5,learning_rate=0.3, max_depth=10,\n",
    "                                n_estimators=350, subsample=0.5, eval_metric='logloss') \n",
    "}\n",
    "\n",
    "models_config = {\n",
    "    'Tuned_RF_All_Features': ['Fwd IAT Total', 'Fwd Packet Length Max', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "                              'Bwd Packets Length Total', 'Bwd Packet Length Max', 'Packet Length Max', 'Packet Length Mean',\n",
    "                              'Packet Length Std', 'Packet Length Variance', 'Avg Packet Size', 'Fwd Header Length',\n",
    "                              'Avg Fwd Segment Size', 'Avg Bwd Segment Size'],\n",
    "    'Tuned_RF_Selected_Features': ['Fwd IAT Total', 'Bwd Packet Length Std', 'Bwd Packets Length Total',\n",
    "                                   'Packet Length Mean', 'Avg Packet Size', 'Fwd Header Length', 'Avg Fwd Segment Size',\n",
    "                                   'Avg Bwd Segment Size'],\n",
    "    'Tuned_XGB_Permutation_Features': ['Fwd IAT Total', 'Fwd Packet Length Max', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "                                       'Packet Length Std', 'Avg Packet Size', 'Fwd Header Length'],\n",
    "    'Tuned_XGB_RFECV_Features': ['Fwd IAT Total', 'Fwd Packet Length Max', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
    "                                 'Bwd Packets Length Total', 'Bwd Packet Length Max', 'Packet Length Max', 'Packet Length Mean',\n",
    "                                 'Packet Length Std', 'Packet Length Variance', 'Avg Packet Size', 'Fwd Header Length', 'Avg Fwd Segment Size']\n",
    "}\n",
    "\n",
    "# Iterating over configurations\n",
    "for model_name, features in models_config.items():\n",
    "    # Extract the base model name (before the underscore)\n",
    "    base_model_name = '_'.join(model_name.split('_')[:2])\n",
    "    process_model(models[base_model_name], features, X_train, y_train, X_val, y_val, X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare model vs test set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from joblib import load\n",
    "# load threshold from eval\n",
    "\n",
    "\n",
    "# Assuming 'top_models' DataFrame contains the paths or identifiers for your models\n",
    "for _, row in top_models.iterrows():\n",
    "    full_model_name = row['model']\n",
    "    # Assuming the model name ends with '_1' and you want to remove this part\n",
    "    model_name = \"_\".join(full_model_name.split('_')[:-1])  # Remove the last part after the last underscore\n",
    "    \n",
    "    model_path = f'../models/{model_name}.joblib'\n",
    "    model = load(model_path)\n",
    "    \n",
    "    # Generate probability predictions\n",
    "    probabilities = model.predict_proba(X_test)[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, probabilities)\n",
    "    \n",
    "    # Calculate F2 score for each threshold\n",
    "    beta = 2\n",
    "    f_scores = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n",
    "    f_scores = np.nan_to_num(f_scores)  # Replace NaNs with zeros for safe max index finding\n",
    "    best_idx = np.argmax(f_scores)  # Find index of the maximum F2 score\n",
    "    best_threshold = thresholds[best_idx]  # Best threshold for maximum F2 score\n",
    "\n",
    "    predictions = (probabilities >= best_threshold).astype(int)\n",
    "    # Append predictions to the test DataFrame\n",
    "    test_df[f'pred_{model_name}'] = predictions\n",
    "\n",
    "# Now test_df has new columns for each model's predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract model names from the top_models DataFrame assuming the names are stored in a column named 'model'\n",
    "model_names = top_models['model'].tolist()\n",
    "\n",
    "# Filter for cases where the 'Attack' column is 1 and any model predicted 0\n",
    "failed_cases = test_df[(test_df['Attack'] == 1) & (test_df.apply(lambda row: any(row[f'pred_{model_name}'] == 0 for model_name in model_names), axis=1))]\n",
    "\n",
    "# Print the number of failed cases\n",
    "print(f\"Number of failed cases: {len(failed_cases)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say the true labels column is named 'true_labels'\n",
    "# Display cases where all models failed to predict correctly\n",
    "failed_cases = test_df[test_df.apply(lambda row: all(row[f'pred_{model_name}'] != row['true_labels'] for model_name in top_models['model']), axis=1)]\n",
    "print(failed_cases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example: Plotting the first few instances where predictions differ\n",
    "subset_df = test_df[test_df['true_labels'] != test_df[f'pred_{top_models.iloc[0][\"model\"]}']].head(10)  # Just an example to grab a few differing predictions\n",
    "sns.barplot(data=subset_df, y='true_labels', x=subset_df.columns[1:], palette='viridis')\n",
    "plt.title('Comparison of Predictions vs. True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate value counts for the failed cases\n",
    "failed_label_counts = failed_cases['Label'].value_counts()\n",
    "\n",
    "# Calculate value counts for the entire test set\n",
    "overall_label_counts = test_df['Label'].value_counts()\n",
    "\n",
    "# Normalize value counts to compare proportions\n",
    "failed_label_proportions = failed_cases['Label'].value_counts(normalize=True)\n",
    "overall_label_proportions = test_df['Label'].value_counts(normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Failed Case Label Counts:\")\n",
    "print(failed_label_counts)\n",
    "print(\"\\nOverall Test Set Label Counts:\")\n",
    "print(overall_label_counts)\n",
    "\n",
    "print(\"\\nFailed Case Label Proportions:\")\n",
    "print(failed_label_proportions)\n",
    "print(\"\\nOverall Test Set Label Proportions:\")\n",
    "print(overall_label_proportions)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
